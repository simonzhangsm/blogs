---
title: 《决策树算法的Python实现》笔记
date: 2016-08-12 11:10:13
tags: [maching-learning, python, decision-tree, 笔记]
---

> 参照：
> 
> [1]: <https://zhuanlan.zhihu.com/p/20794583> "决策树算法的Python实现"
> 
> [2]: <http://m.blog.csdn.net/article/details?id=51344905> "决策树的生成与剪枝"
> 
> [3]: <http://www.chawenti.com/articles/18892.html> "决策树分类算法"
>
> [4]: <http://www.cnblogs.com/zhangchaoyang/articles/2709922.html> "决策树CART"
> 
> 决策树是一种基本的`分类`与`回归`方法，它可以看作if-then规则的集合，也可以认为是定义在`特征空间`与`类空间`上的`条件概率分布`

性质: 每一个样本能且只能被一条路径所覆盖

优点：

* 直观、容易理解、符合人类思维习惯
* 模型可以通过树的形式可视化展示
* 可以直接处理非数值数据

缺点：

* 对于有大量数值输入和输出问题，未必是好的选择
* 不适合变量关系复杂的模型
* 不适合多变量复杂组合的分类模型
* 模型不稳健，节点小变化可能导致整颗树大变化

算法构成：

* 特征选择
* 决策树生成
* 决策树剪枝

if-then规则转换过程：

1. 由从根节点到叶子的每一天路径构建一条规则
2. 路径内节点的特征对应规则的条件
3. 叶子的类最为规则结论

相关算法：

- Bagging
- 随机森林
- Boosting

## 特征选择

判断特征分类能力的准则

> 对训练数据集D，计算其每个特征的信息增益／增益比，并比较它们的大小，从而选择信息增益最大的特征。

信息增益：

\\[g(D, A) = H(D) - H(D | A) \\]

条件增益：

\\[g_R(D|A) = \frac{g(D, A)}{H_A(D)}\\]

其中：
\\[\begin{aligned}
& H(D) =  -\sum\_{k = 1}^K  \frac{|C_k|}{|D|} log\frac{|C\_k|}{|D|} \\\
& H(D|A) = -\sum\_{i = 1}^n\frac{|D\_i|}{|D|} H(D\_i) = -\sum\_{i = 1}^n\frac{|D\_i|}{|D|} \sum\_{k=1}^K \frac{|D\_{ik}|}{|D\_i|} log\frac{|D\_{ik}|}{|D\_i|} \\\
& H\_A(D) = -\sum\_{i = 1}^n \frac{|D\_i|}{|D|} log\frac{|D\_i|}{|D|} \\\
& D 为训练数据集，样本容量为 |D| \\\
& 最终分为K个类别C\_k，各类别样本个数|C\_k| \\\
& 某一特征A有n个不同的取值a\_1,a\_2,...,a\_n \\\
& D根据特征A的取值可将数据集D划分为n个子集D\_1,D\_2,...,D\_n, |D\_i|为D_i的样本个数 \\\
& 子集D\_i中属于类C\_k中的样本集合为D\_{ik}，其样本个数为|D\_{ik}| \\\
\end{aligned}\\]

> 基础知识：
> 
> 设X是一个有限状态的离散型随机变量，其概率分布为
> 
> \\[P(X = x\_i) = p\_i, i = 1, 2,...,n\\]
> 熵：熵(entropy)用于表示**随机变量不确定性的度量**，熵越大，则随机变量的不确定性越大。
> 
> \\[ H\left(X\right) = -\sum\_{x = 1}^n p\_i log\left(p\_i \right)\\]
>
> 条件熵：得知特征X的信息而使得类Y的信息的不确定性减少的程度
> \\[ H(Y\,|\,X) = \sum_{i = 1}^n p\_i\,H(Y\,|\,X = x\_i)\\]

## 决策树生成

实现算法：

* ID3
* C4.5
* CART

### ID3&C4.5

1. 从根节点开始，对各节点计算所有特征的信息增益（C4.5计算增益比例），选择增益最大（C4.5增益比例最大）的特征作为节点特征，并由该特征的不同取值（根据特征分类的样本子集）构建子节点
2. 对子节点递归调研执行1(样本)，构建决策树
3. 直到所有特征的增益信息均很小或者没有特征可选为止

### Classification and Regression Tree(CART)

构建一个二叉决策树，递归二分每个特征，将特征空间划分为有限个单元，并在这些单元确定预测的概率分布。

* 采用`平方误差`最小化准则来评价回归树的预测误差
* 采用`基尼指数`最小化准则来评价分类树的分类能力

基尼指数：

\\[ Gini(p) = \sum\_{k=1}^K p\_k (1 - p\_k) = 1 - \sum\_{k=1}^K p\_k^2 \\]

其中：

\\[
K 为类别个数，P\_k为样本属于C\_k的概率
\\]


对于二类分类问题，基尼指数为：

\\[ Gini(p) = 2p(1-p)\\]

其中：
\\[
p为样本属于分类1的概率
\\]
### 备注

计算的数据约定：

* 每行对应一个样本数据
* 每列对应一个样本特征

二叉决策树的结构：

| 字段 | 描述 |
| ------ | ------ |
| col | 待检验的判断条件所对应的列索引值 |
| value | 对应于为了使结果为True，当前列必须匹配的值 |
| results | 保存的是针对当前分支的结果，它是一个字典 |
| tb | 对应于结果为true时，相对于当前节点的子树上的节点 | 
| fb | 对应于结果为false时，相对于当前节点的子树上的节点 |


## 决策树的剪枝

决策树剪枝损失函数：

\\[C\_\alpha(T) = F(T) + C(T) = \sum_{t=1}^{|T|} N\_t H\_t(T) + \alpha\,|T| \\]

拟合度：

\\[F(T) = \sum_{t=1}^{|T|} N\_t H\_t(T) \\]

模型复杂度惩罚：

\\[C(T) = \alpha\,|T|\\]

某叶子节点的经验熵：

\\[H\_t(T) = -\sum\_{k=1}^K \frac{N\_{tk}}{N\_t} log(\frac{N\_{tk}}{N\_t})\\]

其中：

\\[ \begin{aligned}
T：& 决策树 \\\
|T|：& 决策树T的叶子节点个数\\\
t：& 树T的某叶子节点 \\\
N\_t：& 该叶子的样本个数 \\\
N\_{tk}：& 叶子节点t中k类样本个数,k＝1,2,...,K \\\
\alpha：& 模型复杂度的折衷因子 \\\
\end{aligned}\\]

算法过程：

1. 计算各节点的经验熵
2. 递归地从叶子向上回溯，如果将某节点和所以的叶子合并，其损失函数值减小，则进行剪枝
3. 继续2直到不能合并或根节点为止




